{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.preprocess import clean_str, build_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import CNN1d, binary_accuracy, train, evaluate, epoch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data import TabularDataset, Field, LabelField, BucketIterator\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "TEXT = Field(sequential = True, # text: sequential data\n",
    "             tokenize = str.split, \n",
    "             batch_first = True, \n",
    "             fix_length = 56, # padding size: max length of data text\n",
    "             lower = True)\n",
    "LABEL = LabelField(sequential = False,\n",
    "                   dtype = torch.float)\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format('./model/GoogleNews-vectors-negative300.bin.gz', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the kfold 0 data\n",
      "save the kfold 1 data\n",
      "save the kfold 2 data\n",
      "save the kfold 3 data\n",
      "save the kfold 4 data\n",
      "save the kfold 5 data\n",
      "save the kfold 6 data\n",
      "save the kfold 7 data\n",
      "save the kfold 8 data\n",
      "save the kfold 9 data\n"
     ]
    }
   ],
   "source": [
    "# make dataset for 10-fold\n",
    "data_dir = './preprocess'\n",
    "train_paths, val_paths = build_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "test_acc_lists = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kfold 0\n",
      "\tEpoch: 01 | Epoch Time: 0m 48s\n",
      "\t\tTrain Loss: 0.557 | Train Acc: 70.39%\n",
      "\t\tTest. Loss: 0.440 |  Val. Acc: 78.95%\n",
      "\tEpoch: 02 | Epoch Time: 0m 51s\n",
      "\t\tTrain Loss: 0.434 | Train Acc: 79.65%\n",
      "\t\tTest. Loss: 0.415 |  Val. Acc: 78.86%\n",
      "\tEpoch: 03 | Epoch Time: 0m 51s\n",
      "\t\tTrain Loss: 0.349 | Train Acc: 84.71%\n",
      "\t\tTest. Loss: 0.424 |  Val. Acc: 80.95%\n",
      "\tEpoch: 04 | Epoch Time: 0m 51s\n",
      "\t\tTrain Loss: 0.259 | Train Acc: 88.92%\n",
      "\t\tTest. Loss: 0.469 |  Val. Acc: 81.24%\n",
      "\tEpoch: 05 | Epoch Time: 0m 51s\n",
      "\t\tTrain Loss: 0.188 | Train Acc: 92.96%\n",
      "\t\tTest. Loss: 0.483 |  Val. Acc: 81.05%\n",
      "\tEpoch: 06 | Epoch Time: 0m 52s\n",
      "\t\tTrain Loss: 0.116 | Train Acc: 95.84%\n",
      "\t\tTest. Loss: 0.655 |  Val. Acc: 77.71%\n",
      "\tEpoch: 07 | Epoch Time: 0m 51s\n",
      "\t\tTrain Loss: 0.068 | Train Acc: 97.77%\n",
      "\t\tTest. Loss: 0.513 |  Val. Acc: 80.48%\n",
      "\tEpoch: 08 | Epoch Time: 0m 52s\n",
      "\t\tTrain Loss: 0.040 | Train Acc: 98.99%\n",
      "\t\tTest. Loss: 0.651 |  Val. Acc: 78.95%\n",
      "\tEpoch: 09 | Epoch Time: 0m 52s\n",
      "\t\tTrain Loss: 0.023 | Train Acc: 99.51%\n",
      "\t\tTest. Loss: 0.599 |  Val. Acc: 81.62%\n",
      "\tEpoch: 10 | Epoch Time: 0m 51s\n",
      "\t\tTrain Loss: 0.015 | Train Acc: 99.74%\n",
      "\t\tTest. Loss: 0.635 |  Val. Acc: 82.19%\n",
      "============== last test accuracy: 0.821904752935682\n",
      "\n",
      "kfold 1\n",
      "\tEpoch: 01 | Epoch Time: 0m 55s\n",
      "\t\tTrain Loss: 0.551 | Train Acc: 71.01%\n",
      "\t\tTest. Loss: 0.506 |  Val. Acc: 76.22%\n",
      "\tEpoch: 02 | Epoch Time: 0m 54s\n",
      "\t\tTrain Loss: 0.419 | Train Acc: 80.52%\n",
      "\t\tTest. Loss: 0.506 |  Val. Acc: 75.45%\n",
      "\tEpoch: 03 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.334 | Train Acc: 85.76%\n",
      "\t\tTest. Loss: 0.550 |  Val. Acc: 75.55%\n",
      "\tEpoch: 04 | Epoch Time: 0m 53s\n",
      "\t\tTrain Loss: 0.256 | Train Acc: 89.66%\n",
      "\t\tTest. Loss: 0.463 |  Val. Acc: 79.97%\n",
      "\tEpoch: 05 | Epoch Time: 0m 51s\n",
      "\t\tTrain Loss: 0.171 | Train Acc: 93.66%\n",
      "\t\tTest. Loss: 0.638 |  Val. Acc: 75.92%\n",
      "\tEpoch: 06 | Epoch Time: 0m 52s\n",
      "\t\tTrain Loss: 0.108 | Train Acc: 96.15%\n",
      "\t\tTest. Loss: 1.197 |  Val. Acc: 68.90%\n",
      "\tEpoch: 07 | Epoch Time: 0m 51s\n",
      "\t\tTrain Loss: 0.065 | Train Acc: 98.04%\n",
      "\t\tTest. Loss: 0.618 |  Val. Acc: 77.30%\n",
      "\tEpoch: 08 | Epoch Time: 0m 51s\n",
      "\t\tTrain Loss: 0.035 | Train Acc: 99.15%\n",
      "\t\tTest. Loss: 0.666 |  Val. Acc: 79.39%\n",
      "\tEpoch: 09 | Epoch Time: 0m 50s\n",
      "\t\tTrain Loss: 0.022 | Train Acc: 99.52%\n",
      "\t\tTest. Loss: 0.750 |  Val. Acc: 78.55%\n",
      "\tEpoch: 10 | Epoch Time: 0m 51s\n",
      "\t\tTrain Loss: 0.015 | Train Acc: 99.67%\n",
      "\t\tTest. Loss: 0.777 |  Val. Acc: 79.27%\n",
      "============== last test accuracy: 0.7987674957229978\n",
      "\n",
      "kfold 2\n",
      "\tEpoch: 01 | Epoch Time: 0m 51s\n",
      "\t\tTrain Loss: 0.550 | Train Acc: 70.94%\n",
      "\t\tTest. Loss: 0.685 |  Val. Acc: 62.52%\n",
      "\tEpoch: 02 | Epoch Time: 0m 51s\n",
      "\t\tTrain Loss: 0.424 | Train Acc: 80.15%\n",
      "\t\tTest. Loss: 0.474 |  Val. Acc: 76.81%\n",
      "\tEpoch: 03 | Epoch Time: 0m 58s\n",
      "\t\tTrain Loss: 0.339 | Train Acc: 84.23%\n",
      "\t\tTest. Loss: 0.454 |  Val. Acc: 79.19%\n",
      "\tEpoch: 04 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.260 | Train Acc: 89.35%\n",
      "\t\tTest. Loss: 0.528 |  Val. Acc: 77.42%\n",
      "\tEpoch: 05 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.169 | Train Acc: 93.72%\n",
      "\t\tTest. Loss: 0.658 |  Val. Acc: 76.12%\n",
      "\tEpoch: 06 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.112 | Train Acc: 96.00%\n",
      "\t\tTest. Loss: 0.609 |  Val. Acc: 79.31%\n",
      "\tEpoch: 07 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.066 | Train Acc: 97.93%\n",
      "\t\tTest. Loss: 0.610 |  Val. Acc: 80.30%\n",
      "\tEpoch: 08 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.042 | Train Acc: 98.73%\n",
      "\t\tTest. Loss: 0.703 |  Val. Acc: 80.11%\n",
      "\tEpoch: 09 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.026 | Train Acc: 99.27%\n",
      "\t\tTest. Loss: 0.813 |  Val. Acc: 78.96%\n",
      "\tEpoch: 10 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.017 | Train Acc: 99.64%\n",
      "\t\tTest. Loss: 0.822 |  Val. Acc: 79.73%\n",
      "============== last test accuracy: 0.8052173824537368\n",
      "\n",
      "kfold 3\n",
      "\tEpoch: 01 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.556 | Train Acc: 70.29%\n",
      "\t\tTest. Loss: 0.456 |  Val. Acc: 78.03%\n",
      "\tEpoch: 02 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.424 | Train Acc: 80.68%\n",
      "\t\tTest. Loss: 0.441 |  Val. Acc: 79.26%\n",
      "\tEpoch: 03 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.342 | Train Acc: 84.78%\n",
      "\t\tTest. Loss: 0.457 |  Val. Acc: 78.53%\n",
      "\tEpoch: 04 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.262 | Train Acc: 89.05%\n",
      "\t\tTest. Loss: 0.513 |  Val. Acc: 78.18%\n",
      "\tEpoch: 05 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.172 | Train Acc: 93.37%\n",
      "\t\tTest. Loss: 0.597 |  Val. Acc: 77.85%\n",
      "\tEpoch: 06 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.114 | Train Acc: 96.12%\n",
      "\t\tTest. Loss: 0.536 |  Val. Acc: 80.46%\n",
      "\tEpoch: 07 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.059 | Train Acc: 98.42%\n",
      "\t\tTest. Loss: 0.615 |  Val. Acc: 80.81%\n",
      "\tEpoch: 08 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.031 | Train Acc: 99.31%\n",
      "\t\tTest. Loss: 0.650 |  Val. Acc: 80.53%\n",
      "\tEpoch: 09 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.026 | Train Acc: 99.46%\n",
      "\t\tTest. Loss: 0.743 |  Val. Acc: 80.62%\n",
      "\tEpoch: 10 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.013 | Train Acc: 99.82%\n",
      "\t\tTest. Loss: 0.809 |  Val. Acc: 79.92%\n",
      "============== last test accuracy: 0.8094202876091003\n",
      "\n",
      "kfold 4\n",
      "\tEpoch: 01 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.558 | Train Acc: 70.56%\n",
      "\t\tTest. Loss: 0.449 |  Val. Acc: 79.63%\n",
      "\tEpoch: 02 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.425 | Train Acc: 80.38%\n",
      "\t\tTest. Loss: 0.427 |  Val. Acc: 80.34%\n",
      "\tEpoch: 03 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.340 | Train Acc: 85.32%\n",
      "\t\tTest. Loss: 0.425 |  Val. Acc: 81.03%\n",
      "\tEpoch: 04 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.267 | Train Acc: 88.44%\n",
      "\t\tTest. Loss: 0.465 |  Val. Acc: 79.98%\n",
      "\tEpoch: 05 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.171 | Train Acc: 93.73%\n",
      "\t\tTest. Loss: 0.547 |  Val. Acc: 78.42%\n",
      "\tEpoch: 06 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.116 | Train Acc: 95.97%\n",
      "\t\tTest. Loss: 0.517 |  Val. Acc: 80.56%\n",
      "\tEpoch: 07 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.062 | Train Acc: 98.15%\n",
      "\t\tTest. Loss: 0.669 |  Val. Acc: 80.30%\n",
      "\tEpoch: 08 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.045 | Train Acc: 98.85%\n",
      "\t\tTest. Loss: 0.689 |  Val. Acc: 80.88%\n",
      "\tEpoch: 09 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.024 | Train Acc: 99.41%\n",
      "\t\tTest. Loss: 0.720 |  Val. Acc: 81.54%\n",
      "\tEpoch: 10 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.016 | Train Acc: 99.70%\n",
      "\t\tTest. Loss: 0.755 |  Val. Acc: 81.09%\n",
      "============== last test accuracy: 0.8169924758729481\n",
      "\n",
      "kfold 5\n",
      "\tEpoch: 01 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.557 | Train Acc: 70.02%\n",
      "\t\tTest. Loss: 0.478 |  Val. Acc: 77.16%\n",
      "\tEpoch: 02 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.420 | Train Acc: 81.13%\n",
      "\t\tTest. Loss: 0.489 |  Val. Acc: 75.91%\n",
      "\tEpoch: 03 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.342 | Train Acc: 84.77%\n",
      "\t\tTest. Loss: 0.452 |  Val. Acc: 79.45%\n",
      "\tEpoch: 04 | Epoch Time: 0m 58s\n",
      "\t\tTrain Loss: 0.262 | Train Acc: 88.82%\n",
      "\t\tTest. Loss: 0.465 |  Val. Acc: 79.21%\n",
      "\tEpoch: 05 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.177 | Train Acc: 93.08%\n",
      "\t\tTest. Loss: 0.766 |  Val. Acc: 72.41%\n",
      "\tEpoch: 06 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.110 | Train Acc: 96.28%\n",
      "\t\tTest. Loss: 0.569 |  Val. Acc: 78.74%\n",
      "\tEpoch: 07 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.066 | Train Acc: 98.01%\n",
      "\t\tTest. Loss: 0.576 |  Val. Acc: 81.12%\n",
      "\tEpoch: 08 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.040 | Train Acc: 98.92%\n",
      "\t\tTest. Loss: 0.708 |  Val. Acc: 79.32%\n",
      "\tEpoch: 09 | Epoch Time: 0m 57s\n",
      "\t\tTrain Loss: 0.024 | Train Acc: 99.41%\n",
      "\t\tTest. Loss: 0.681 |  Val. Acc: 79.97%\n",
      "\tEpoch: 10 | Epoch Time: 0m 58s\n",
      "\t\tTrain Loss: 0.016 | Train Acc: 99.64%\n",
      "\t\tTest. Loss: 0.774 |  Val. Acc: 80.62%\n",
      "============== last test accuracy: 0.8101185741631881\n",
      "\n",
      "kfold 6\n",
      "\tEpoch: 01 | Epoch Time: 0m 58s\n",
      "\t\tTrain Loss: 0.559 | Train Acc: 69.39%\n",
      "\t\tTest. Loss: 0.497 |  Val. Acc: 75.49%\n",
      "\tEpoch: 02 | Epoch Time: 1m 1s\n",
      "\t\tTrain Loss: 0.427 | Train Acc: 80.24%\n",
      "\t\tTest. Loss: 0.455 |  Val. Acc: 79.28%\n",
      "\tEpoch: 03 | Epoch Time: 1m 1s\n",
      "\t\tTrain Loss: 0.348 | Train Acc: 84.45%\n",
      "\t\tTest. Loss: 0.509 |  Val. Acc: 76.67%\n",
      "\tEpoch: 04 | Epoch Time: 1m 3s\n",
      "\t\tTrain Loss: 0.264 | Train Acc: 89.15%\n",
      "\t\tTest. Loss: 0.540 |  Val. Acc: 76.57%\n",
      "\tEpoch: 05 | Epoch Time: 1m 7s\n",
      "\t\tTrain Loss: 0.172 | Train Acc: 93.37%\n",
      "\t\tTest. Loss: 0.481 |  Val. Acc: 79.61%\n",
      "\tEpoch: 06 | Epoch Time: 1m 3s\n",
      "\t\tTrain Loss: 0.111 | Train Acc: 96.18%\n",
      "\t\tTest. Loss: 0.536 |  Val. Acc: 79.01%\n",
      "\tEpoch: 07 | Epoch Time: 1m 2s\n",
      "\t\tTrain Loss: 0.069 | Train Acc: 97.66%\n",
      "\t\tTest. Loss: 0.569 |  Val. Acc: 79.96%\n",
      "\tEpoch: 08 | Epoch Time: 1m 1s\n",
      "\t\tTrain Loss: 0.039 | Train Acc: 99.00%\n",
      "\t\tTest. Loss: 0.631 |  Val. Acc: 80.66%\n",
      "\tEpoch: 09 | Epoch Time: 1m 1s\n",
      "\t\tTrain Loss: 0.025 | Train Acc: 99.43%\n",
      "\t\tTest. Loss: 0.656 |  Val. Acc: 81.29%\n",
      "\tEpoch: 10 | Epoch Time: 1m 2s\n",
      "\t\tTrain Loss: 0.013 | Train Acc: 99.81%\n",
      "\t\tTest. Loss: 0.740 |  Val. Acc: 80.40%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== last test accuracy: 0.809130425038545\n",
      "\n",
      "kfold 7\n",
      "\tEpoch: 01 | Epoch Time: 1m 3s\n",
      "\t\tTrain Loss: 0.553 | Train Acc: 70.64%\n",
      "\t\tTest. Loss: 0.491 |  Val. Acc: 74.63%\n",
      "\tEpoch: 02 | Epoch Time: 1m 4s\n",
      "\t\tTrain Loss: 0.421 | Train Acc: 80.89%\n",
      "\t\tTest. Loss: 0.537 |  Val. Acc: 72.00%\n",
      "\tEpoch: 03 | Epoch Time: 1m 6s\n",
      "\t\tTrain Loss: 0.346 | Train Acc: 84.74%\n",
      "\t\tTest. Loss: 0.446 |  Val. Acc: 79.08%\n",
      "\tEpoch: 04 | Epoch Time: 1m 3s\n",
      "\t\tTrain Loss: 0.260 | Train Acc: 89.44%\n",
      "\t\tTest. Loss: 0.474 |  Val. Acc: 79.17%\n",
      "\tEpoch: 05 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.184 | Train Acc: 93.01%\n",
      "\t\tTest. Loss: 0.506 |  Val. Acc: 79.82%\n",
      "\tEpoch: 06 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.107 | Train Acc: 95.91%\n",
      "\t\tTest. Loss: 0.553 |  Val. Acc: 79.56%\n",
      "\tEpoch: 07 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.066 | Train Acc: 97.92%\n",
      "\t\tTest. Loss: 0.584 |  Val. Acc: 80.07%\n",
      "\tEpoch: 08 | Epoch Time: 0m 58s\n",
      "\t\tTrain Loss: 0.039 | Train Acc: 98.96%\n",
      "\t\tTest. Loss: 0.632 |  Val. Acc: 79.93%\n",
      "\tEpoch: 09 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.022 | Train Acc: 99.53%\n",
      "\t\tTest. Loss: 0.693 |  Val. Acc: 79.73%\n",
      "\tEpoch: 10 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.014 | Train Acc: 99.74%\n",
      "\t\tTest. Loss: 0.717 |  Val. Acc: 80.86%\n",
      "============== last test accuracy: 0.8091925467763629\n",
      "\n",
      "kfold 8\n",
      "\tEpoch: 01 | Epoch Time: 0m 58s\n",
      "\t\tTrain Loss: 0.549 | Train Acc: 70.79%\n",
      "\t\tTest. Loss: 0.463 |  Val. Acc: 77.65%\n",
      "\tEpoch: 02 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.420 | Train Acc: 80.76%\n",
      "\t\tTest. Loss: 0.436 |  Val. Acc: 79.24%\n",
      "\tEpoch: 03 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.341 | Train Acc: 85.44%\n",
      "\t\tTest. Loss: 0.443 |  Val. Acc: 79.45%\n",
      "\tEpoch: 04 | Epoch Time: 0m 57s\n",
      "\t\tTrain Loss: 0.259 | Train Acc: 89.04%\n",
      "\t\tTest. Loss: 0.463 |  Val. Acc: 80.08%\n",
      "\tEpoch: 05 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.173 | Train Acc: 93.39%\n",
      "\t\tTest. Loss: 0.481 |  Val. Acc: 79.52%\n",
      "\tEpoch: 06 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.111 | Train Acc: 96.04%\n",
      "\t\tTest. Loss: 0.543 |  Val. Acc: 80.42%\n",
      "\tEpoch: 07 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.064 | Train Acc: 98.07%\n",
      "\t\tTest. Loss: 0.673 |  Val. Acc: 78.93%\n",
      "\tEpoch: 08 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.040 | Train Acc: 98.85%\n",
      "\t\tTest. Loss: 0.630 |  Val. Acc: 80.99%\n",
      "\tEpoch: 09 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.020 | Train Acc: 99.69%\n",
      "\t\tTest. Loss: 0.710 |  Val. Acc: 79.57%\n",
      "\tEpoch: 10 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.015 | Train Acc: 99.70%\n",
      "\t\tTest. Loss: 0.748 |  Val. Acc: 80.97%\n",
      "============== last test accuracy: 0.8098989860578016\n",
      "\n",
      "kfold 9\n",
      "\tEpoch: 01 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.554 | Train Acc: 70.72%\n",
      "\t\tTest. Loss: 0.455 |  Val. Acc: 79.19%\n",
      "\tEpoch: 02 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.432 | Train Acc: 80.22%\n",
      "\t\tTest. Loss: 0.444 |  Val. Acc: 79.48%\n",
      "\tEpoch: 03 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.340 | Train Acc: 85.18%\n",
      "\t\tTest. Loss: 0.464 |  Val. Acc: 77.47%\n",
      "\tEpoch: 04 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.264 | Train Acc: 89.05%\n",
      "\t\tTest. Loss: 0.521 |  Val. Acc: 76.57%\n",
      "\tEpoch: 05 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.176 | Train Acc: 93.16%\n",
      "\t\tTest. Loss: 0.524 |  Val. Acc: 79.02%\n",
      "\tEpoch: 06 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.109 | Train Acc: 96.16%\n",
      "\t\tTest. Loss: 0.502 |  Val. Acc: 81.29%\n",
      "\tEpoch: 07 | Epoch Time: 0m 59s\n",
      "\t\tTrain Loss: 0.060 | Train Acc: 98.26%\n",
      "\t\tTest. Loss: 0.559 |  Val. Acc: 81.65%\n",
      "\tEpoch: 08 | Epoch Time: 0m 58s\n",
      "\t\tTrain Loss: 0.035 | Train Acc: 99.14%\n",
      "\t\tTest. Loss: 0.620 |  Val. Acc: 81.11%\n",
      "\tEpoch: 09 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.022 | Train Acc: 99.63%\n",
      "\t\tTest. Loss: 0.653 |  Val. Acc: 80.75%\n",
      "\tEpoch: 10 | Epoch Time: 1m 0s\n",
      "\t\tTrain Loss: 0.017 | Train Acc: 99.58%\n",
      "\t\tTest. Loss: 0.723 |  Val. Acc: 81.29%\n",
      "============== last test accuracy: 0.814705881205472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for kfold in range(10):\n",
    "    # make datasets\n",
    "    train_path = train_paths[kfold]\n",
    "    val_path = val_paths[kfold]\n",
    "    train_data = TabularDataset(path= train_path, skip_header = True,\n",
    "        format='csv', fields=[('label', LABEL), ('text', TEXT)])\n",
    "    test_data = TabularDataset(path= val_path, skip_header = True,\n",
    "        format='csv', fields=[('label', LABEL), ('text', TEXT)])\n",
    "\n",
    "    TEXT.build_vocab(train_data)\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    # for pretrained embedding vectors\n",
    "    w2v_vectors = []\n",
    "    for token, idx in TEXT.vocab.stoi.items():\n",
    "        # pad token -> zero \n",
    "        if idx == 1:\n",
    "            w2v_vectors.append(torch.zeros(EMBEDDING_DIM))\n",
    "        # if word in word2vec vocab -> replace with pretrained word2vec\n",
    "        elif token in w2v.wv.vocab.keys():\n",
    "            w2v_vectors.append(torch.FloatTensor(w2v[token]))\n",
    "        # oov -> randomly initialized uniform distribution\n",
    "        else: \n",
    "            w2v_vectors.append(torch.distributions.Uniform(-0.25, +0.25).sample((EMBEDDING_DIM,)))\n",
    "\n",
    "    TEXT.vocab.set_vectors(TEXT.vocab.stoi, w2v_vectors, EMBEDDING_DIM)\n",
    "    pretrained_embeddings = torch.FloatTensor(TEXT.vocab.vectors)\n",
    "\n",
    "    # make iterators\n",
    "    train_iterator,  test_iterator = BucketIterator.splits(\n",
    "        (train_data, test_data), \n",
    "        batch_size = BATCH_SIZE, \n",
    "        device = device, sort=False, shuffle = True)\n",
    "    \n",
    "    # define a model\n",
    "    INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "    model = CNN1d(pretrained_embeddings, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
    "    optimizer = optim.Adadelta(model.parameters(), rho=0.95)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    # train\n",
    "    best_test_acc = -float('inf')\n",
    "    model_name= './model/model' + str(kfold) + '.pt'\n",
    "    print('kfold', kfold)\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "        test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "\n",
    "        \n",
    "        print(f'\\tEpoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\t\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t\\tTest. Loss: {test_loss:.3f} |  Val. Acc: {test_acc*100:.2f}%')\n",
    "    \n",
    "\n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "\n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    test_acc_lists.append(test_acc)\n",
    "    print(f'============== last test accuracy: {test_acc}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== 10 - fold test accuracy ==============\n",
      "Mean acc 81.01%\n",
      "[0.821313121102073, 0.7948148142207753, 0.821904752935682, 0.7987674957229978, 0.8052173824537368, 0.8094202876091003, 0.8169924758729481, 0.8101185741631881, 0.809130425038545, 0.8091925467763629, 0.8098989860578016, 0.814705881205472]\n"
     ]
    }
   ],
   "source": [
    "print('============== 10 - fold test accuracy ==============')\n",
    "print(f'Mean acc {np.mean(test_acc_lists)  * 100 :.2f}%')\n",
    "print(test_acc_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
